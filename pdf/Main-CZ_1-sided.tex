%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,oneside,american,czech]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=4cm,bmargin=3cm,lmargin=3cm,rmargin=2cm,headheight=0.8cm,headsep=1cm,footskip=0.5cm}
\pagestyle{headings}
\setcounter{secnumdepth}{3}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{libertine}
\usepackage{comment}
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxlist}[1]
	{\begin{list}{}
		{\settowidth{\labelwidth}{#1}
		 \setlength{\leftmargin}{\labelwidth}
		 \addtolength{\leftmargin}{\labelsep}
		 \renewcommand{\makelabel}[1]{##1\hfil}}}
	{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\R}{\mathbb{R}}
\newtheorem{definition}{Definice}[section]
\newtheorem{vet}{Věta}[section]
\newtheorem*{poz}{Poznámka}
\newtheorem*{pří}{Příklad}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
%  Matematika
\newcommand{\ee}{\mathrm{e}} %eulerovo číslo
\newcommand{\ii}{\mathrm{i}} %imaginární jednotka
\newcommand{\inR}{\in \mathbb{R}}
\newcommand{\dd}{\mathrm{d}}
\newcommand\KLDeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny KLD}}}{\approx}}}
\newcommand*{\matr}[1]{\mathbfit{#1}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\conj}[1]{\overline{#1}}
\newcommand*{\hermconj}{^{\mathsf{H}}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmin}{arg\,min} 
% Jednotky
\newcommand{\unit}[1]{\,\mathrm{#1}} %jednotky zadávejte pomocí tohoto příkazu
\renewcommand{\deg}{\ensuremath{\mathring{\;}}} %symbol stupně
\newcommand{\celsius}{\ensuremath{\deg\mathrm{C}}} %stupně celsia

%(hodnota plus mínus chyba) jednotka
\newcommand{\hodn}[3]{(#1 \pm #2)\unit{#3}} 

%veličina [jednotka] do hlavičky tabulky
\newcommand{\tabh}[2]{\ensuremath{#1\,[\mathrm{#2}]}} 
% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}

\makeatother

\usepackage{babel}
\begin{document}
\def\documentdate{7. \v{c}ervence 2020}

%%\def\documentdate{\today}

\pagestyle{empty}
{\centering

\noindent %
\begin{minipage}[c]{3cm}%
\noindent \begin{center}
\includegraphics[width=3cm,height=3cm,keepaspectratio]{Images/TITLE/cvut}
\par\end{center}%
\end{minipage}%
\begin{minipage}[c]{0.6\linewidth}%
\begin{center}
\textsc{\large{}České vysoké učení technické v Praze}{\large{}}\\
{\large{}Fakulta jaderná a fyzikálně inženýrská}
\par\end{center}%
\end{minipage}%
\begin{minipage}[c]{3cm}%
\noindent \begin{center}
\includegraphics[width=3cm,height=3cm,keepaspectratio]{Images/TITLE/fjfi}
\par\end{center}%
\end{minipage}

\vspace{3cm}

\textbf{\huge{}Generativní modely dat popsaných stromovou strukturou}{\huge\par}

\vspace{1cm}

\selectlanguage{american}%
\textbf{\huge{}Generative models of tree structured data}{\huge\par}

\selectlanguage{czech}%
\vspace{2cm}

{\large{}Bakalářská práce}{\large\par}

}

\vfill{}

\begin{lyxlist}{MMMMMMMMM}
\begin{singlespace}
\item [{Autor:}] \textbf{Jakub Bureš}
\item [{Vedoucí~práce:}] \textbf{Doc. Ing. Václav Šmídl, Ph.D.}
\item [{Konzultant:}] \textbf{Doc. Ing. Tomáš Pevný, Ph.D.}
\item [{Akademický~rok:}] 2019/2020
\end{singlespace}
\end{lyxlist}
\newpage{}

~

\vfill{}

\begin{center}
\begin{enumerate}	
\item Seznamte se s popisem dat pomocí stromové struktury. Zvláštní pozornost věnujte metodám více instančního učení (multiple instance learning). Seznamte se s konceptem vnořeného prostoru (embedded space) a jeho reprezentace pomocí neuronových sítí. 
\item	
Seznamte se se základními generativními modely dat popsaných vektorem příznaků. Zvláštní pozornost věnujte metodám typu autoencoder a jejich variační formě. Demonstrujte vlastnosti modelů na jednoduchých příkladech. V maximální míře využijte dostupné knihovny pro generativní modely.
\item
Navrhněte několik příkladů typů dat se stromovou strukturou a pro každý z nich navrhněte generativní model. Navrhněte algoritmus pro určení jeho parametrů z dat a diskutujte vhodnost jednotlivých architektur neuronových sítí.
\item
Seznamte se s různými druhy apriorních rozložení používaných na latentní proměnné autoencoderu. Odvoďte algoritmy odhadu jejich parametrů a srovnejte jejich výsledky se základním modelem. Diskutujte výsledné odhady.
\item	
Vyvinutou metodu aplikujte na vhodně zvolená reálná data a diskutujte vliv zvoleného apriorního rozložení na výsledky.
\end{enumerate}
\par\end{center}

\vfill{}

~\newpage{}

~

\vfill{}

\begin{center}
- Zadání práce (zadní strana) -
\par\end{center}

\vfill{}

~\newpage{}

\noindent \emph{\Large{}Poděkování:}{\Large\par}

\noindent Chtěl bych zde poděkovat především svému školiteli panu Doc. Ing. Václavu Šmídlovi, Ph.D.
za pečlivost, ochotu, vstřícnost a odborné i lidské zázemí při vedení
mé bakalářské práce. Dále děkuji svému konzultantovi panu Doc. Ing. Tomáši Pevnému, Ph.D.

\vfill

\noindent \emph{\Large{}Čestné prohlášení:}{\Large\par}

\noindent Prohlašuji, že jsem tuto práci vypracoval samostatně a uvedl
jsem všechnu použitou literaturu.

\bigskip{}

\noindent V Praze dne \documentdate\hfill{}Jakub Bureš

\vspace{2cm}

\newpage{}

\begin{onehalfspace}
\noindent \emph{Název práce:} 

\noindent \textbf{Generativní modely dat popsaných stromovou strukturou}
\end{onehalfspace}

\bigskip{}

\noindent \emph{Autor:} Jakub Bureš

\bigskip{}

\noindent \emph{Obor:} Matematické inženýrství
 \bigskip{}

\noindent \emph{Zaměření:} Aplikované matematicko-stochastické metody

\bigskip{}

\noindent \emph{Druh práce:} Bakalářská práce

\bigskip{}

\noindent \emph{Vedoucí práce:} Doc. Ing. Václav Šmídl, Ph.D.\\
ÚTIA AV ČR
Pod vodárenskou věží 4
182 00 Praha 8

\bigskip{}

\noindent \emph{Konzultant:} Doc. Ing. Tomáš Pevný, Ph.D. \\
Katedra počítačů
FEL ČVUT Praha
Technická 1902/2
166 27 Praha 6 - Dejvice
\bigskip{}

\noindent \emph{Abstrakt:} Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. 

\bigskip{}

\noindent \emph{Klíčová slova:} klíčová slova (nebo výrazy) seřazená
podle abecedy a oddělená čárkou

\vfill{}
~

\selectlanguage{american}%
\begin{onehalfspace}
\noindent \emph{Title:}Generative models of tree structured data 

\noindent \textbf{Generative models of tree structured data}
\end{onehalfspace}

\bigskip{}

\noindent \emph{Author:} Jakub Bureš

\bigskip{}

\noindent \emph{Abstract:} Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text. Max. 10 lines of English
abstract text. Max. 10 lines of English abstract text. Max. 10 lines
of English abstract text. Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text. Max. 10 lines of English
abstract text. Max. 10 lines of English abstract text. Max. 10 lines
of English abstract text. Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text. Max. 10 lines of English
abstract text. Max. 10 lines of English abstract text. Max. 10 lines
of English abstract text. Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text.

\bigskip{}

\noindent \emph{Key words:} keywords in alphabetical order separated
by commas

\selectlanguage{czech}%
\newpage{}

\pagestyle{plain}

\tableofcontents{}

\newpage{}





\chapter{Teorie}
\section{Optimalizace}
 Předpokládejme že máme trénovací data obsahující $n$ pozorování $x$, nebo-li $\textbf{X} = (x_1,\dots , x_n)$. Dále máme ke každému $x$ právě jedno pozorování $t$, psáno $\textbf{t} =(t_1,\dots, t_n)$, komplexně zapsáno zobrazením $(x_1,\dots, x_n)~\longmapsto~(t_1,\dots, t_n)$.\\
Naším cílem je najít nejlepší proložení dat, čili fit, pomocí polynomické funkce řádu $n$ ve tvaru
\begin{equation*}
y(x, \theta) = \theta_0 + \theta_1x + \theta_2x^2 + \dots + \theta_nx^n = \sum_{i = 0}^{n} {\theta_{i}x^{i}},
\end{equation*}
která je lineární v neznámých parametrech $\theta = \left( \theta_0, \theta_1,\dots, \theta_n\right)$. Takové modely nazýváme lineární a jejich vlastnosti budeme nadále využívat.\\
Abychom našli ten nejlepší možný fit, je nutno minimalizovat tzv. ztrátovou funkci (loss function) $E(\Theta)$. Tato funkce znázorňuje eukleidovskou vzdálenost od pozorovaných bodů $\textbf{t}$ k hledané funkci $y(x, \theta)$. Je tvaru
\begin{equation}\label{eq:loss}
E(\theta) = \frac{1}{2}\sum_{n = 1}^{N}\left[ y(x_n, \theta) - t_n\right]^2
\end{equation}
Minimalizací ztrátové funkce získáme parametry 
\begin{equation*}
\theta = \argmin_{\theta} E(\theta)
\end{equation*}
a jsme tudíž schopni sestavit předpis polynomu, který nejlépe daná data proloží. Tuto úlohu nazveme optimalizací.\\ V této bakalářské práci budeme výhradně používat gradientní metodu Gradient descent a její vylepšenou verzi ADAM.

\subsection{Gradient Descent}
Jedná se iterativní optimalizační metodu pomocí které hledáme minimum dané funkce. My se snažíme zminimalizovat funkci $L = (\textbf{y}-\mathbb{X}\theta)^{T}(\textbf{y}-\mathbb{X}\theta)$, neboli funkci \eqref{eq:loss} pouze přepsanou pomocí maticového zápisu. Minimalizujeme $L$, tedy derivujeme dle vektoru $\Theta$ a dostaneme
\begin{equation*}
\nabla_{\theta}\ L = 2\mathbb{X}^{T}(\mathbb{X}\theta - \textbf{y}),
\end{equation*}
kde $\nabla_{\theta}$ značí gradient funkce $L$ přes všechny hodnoty $\theta$.
Použijeme bod $\textbf{a}$ funkce $L(\theta)$ jako výchozí bod, ze kterého se pohybujeme ve směru záporného gradientu s krokem $\gamma$~$\in$~$\mathbb{R}_{+}$. Tento postup provádíme, dokud nejsme v minimu funkce a můžeme matematicky zapsat následujícím zápisem:
\begin{equation*}
a_{n+1} = a_{n} - \gamma \nabla_{\theta} L(a_n)
\end{equation*}
\subsection*{ADAM}
Předchozí metoda není při větším množství dat tak rychlá, jak bychom pro výpočet minima funkce potřebovali. Používáme proto adaptivní iterační gradientní metodu ADAM (Adaptive Moment Estimation), která navíc používá druhý moment gradientu. Zatímco Gradient descent má krok stále stejný, u metody ADAM je krok $\gamma$ adaptivní. Popřípadě můžeme ladit i zapomínací koeficienty, což už je ale mimo rámec této práce a my nebudeme při výpočtech využívat.
\pagestyle{headings}
\subsection{Metoda nejmenších čtverců}
Uvažujme předeterminovaný systém $n$ lineárních rovnic a $p$ neznámých parametrech $\theta = (\theta_0, \theta_1,\dots,\theta_p)$

\begin{align*}
\sum_{j = 0}^{p}x_i^j\theta_j = y_i, 
\end{align*}
kde i $\in (0,1,\dots,n)$ a $n>p$. Přepíšeme pomocí maticového zápisu
\begin{equation*}
\begin{pmatrix}
y_1 \\
y_2\\
\vdots \\
y_n
\end{pmatrix}
=
\begin{pmatrix}
1 & x_1 & x_1^2 & \dots & x_1^p \\
1 & x_2 & x_2^2 & \dots & x_2^p \\
\vdots & \ddots & & &\\
1 & x_n & x_n^2 & \dots & x_n^p \\
\end{pmatrix}
\cdot
\begin{pmatrix}
\theta_0 \\
\theta_1 \\
\vdots \\
\theta_p
\end{pmatrix}
+
\begin{pmatrix}
\varepsilon_0 \\
\varepsilon_1 \\
\vdots \\
\varepsilon_p
\end{pmatrix}
\end{equation*}
 Pro jednoduchost budeme tímto maticovým zápisem rozumět následující rovnici
\begin{equation}\label{eq:regresematrix}
\textbf{y} = \mathbb{X} \cdot {\theta} + \epsilon
\end{equation}
Naším cílem je získání parametrů $\theta$, proto obě strany rovnice vynásobíme zleva $ \mathbb{X}^T$. Tím nám rovnice přejde do tvaru
\begin{equation*}
\mathbb{X}^T\cdot\textbf{y} =\mathbb{X}^T\cdot \mathbb{X} \cdot \hat{{\theta}}
\end{equation*}
Teď už stačí rovnici zleva vynásobit inverzní maticí $(\mathbb{X}^T\cdot \mathbb{X})^{-1}$. Dostaneme tak konečné řešení
\begin{equation}\label{regresethetahat}
\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^{T}\textbf{y}
\end{equation}
Nicméně i zde lze parametry $\theta$ odhadovat pomocí gradientní metody a to způsobem, který je popsán rovnicí \eqref{eq:loss}.

\newpage

\section{Úvod do pravděpodobnosti a Bayesovská statistika}
\begin{definition}{(Kolmogorova definice pravděpodobnosti).} 
Mějme množinu $\Omega$ vybavenou $\sigma$-algebrou $\mathcal{A}$, tedy souborem podmnožin obsahujícím $\Omega$ a uzavřeným na doplňky a spočetná sjednocení. Pak libovolnou funkci $P : \mathcal{A} \to \mathbb{R}$ , která splňuje :
\begin{enumerate}
\item $(\forall A\in \mathcal{A})(P(A) \geq 0).$
\item$ P(\Omega) = 1 $
\item $\forall A_j $ disjunktní platí $P(\sum_{j=1}^{\infty}A_j) = \sum_{j=1}^{\infty}P(A_j)$
\end{enumerate}
\end{definition}
\begin{vet}{(Vlastnosti P).} Mějme pravděpodobnostní prostor $(\Omega,\mathcal{A}, P)$ a něcht $(\forall j \in \mathbb{N})(A_j \in \mathcal{A})$ a $B \in \mathcal{A}$. Pak platí:
\begin{enumerate}


\item $P(\emptyset) = 0$,
\item Aditivita:  $P(\sum_{j=1}^{n}A_j) = \sum_{j=1}^{n}P(A_j)$,
\item Monotonie: $A\subset B \Rightarrow P(A) \leq P(B)$,
\item Subtraktivita: $A\subset B \Rightarrow P(B\smallsetminus A) = P(B) - P(A)$,
\item Omezenost: $(\forall A \in \mathcal{A})(P(A)\leqslant 1) $,
\item Komplementarita: $A \in \mathcal{A} \Rightarrow P(A^C) = 1 - P(A)$
\end{enumerate}
\end{vet}
\begin{definition}{(Podmíněná pravděpodobnost).} Nechť $A,B \in \mathcal{A}$ a $P(B)>0$. Pak definujeme podmíněnou pravděpodobnost:
\begin{equation}\label{podminenapravdepodobnost}
P(A|B) = \frac{P(A,B)}{P(B)}
\end{equation}

\end{definition}
\begin{vet}{(Součinové pravidlo).}
Nechť $A_1,\ldots,A_n \in \mathcal{A} $ a dále nechť také $P(A_1,\ldots,A_n ) > 0$. Potom platí:
\begin{equation}\label{chainrule}
P(A_1,\ldots,A_n) = P(A_1)\cdot P(A_2|A_1) \cdot P(A_3| A_2, A_1)\cdot \ldots \cdot P(A_n|A_1,\ldots,A_{n-1})
\end{equation}
\end{vet}
\begin{vet}{(Bayseova věta).} Nechť $A \in \mathcal{A} $ a $P(B) \neq 0$. Potom platí:
\begin{equation}\label{Bayes}
P(A|B) = \frac{P(B\vert A)P(A)}{P(B)}
\end{equation}
\end{vet}
\begin{poz}
$P(A)$ nazýváme prior a $P(A|B)$ nazýváme posterior.
\end{poz}
\begin{vet}{(Nezávislost jevů).} Nechť $A_j \in \mathcal{A} (\forall j \in \mathbb{N})$. Potom jevy nazveme nezávislé právě tehdy když platí podmínka
\begin{equation}
P(A_1,\ldots,A_k) = \prod_{i=1}^{k} P(A_i)
\end{equation}
\end{vet}

\subsection{Hustoty pravděpodobnosti}
Primárním cílem generativního modelování je hledání distribuce nebo-li hustoty pravděpodobnosti daných dat. Výhodou je, že pro hustotu pravděpodobnosti můžeme využívat stejně pravidlo podmíněnosti \eqref{podminenapravdepodobnost}, součinové pravidlo \eqref{chainrule} a Bayeseovo pravidlo \eqref{Bayes}. Toto se pro nás ukáže jako naprosto klíčové. Budeme uvažovat pouze spojitá rozdělení pravděpodobnosti náhodné veličiny $X$.

\begin{definition}{(Hustota pravděpodobnosti).}\label{hustota}
Hustotou pravděpodobnosti náhodné veličiny $X$ rozumíme spojitou funkci $p(x)$, která splňuje  následující dvě podmínky:
\begin{enumerate}
\item  $p(x) \geq 0$
\item $\int_{-\infty}^{\infty}p(x)= 1$
\end{enumerate}
\end{definition}
\begin{poz}
Hustotu pravděpodobnosti lze definovat také pro vícerozměrné funkce $p(\textbf{x})$. Podmínky, které musí vícerozměrná hustota splňovat jsou analogické:
\begin{enumerate}
\item  $p(\textbf{x}) \geq 0$
\item $\int_{-\infty}^{\infty}\cdot \ldots \cdot \int_{-\infty}^{\infty} p(\textbf{x})= 1$
\end{enumerate}
\end{poz}

\begin{definition}{(Střední hodnota náhodné veličiny)}
Má-li náhodná veličina $X$ spojitou hustotu pravděpodobnosti $p(x)$, definujeme její střední (očekávanou) hodnotu  $\mathbb{E}\left[X\right]$, alternativně značeno $\langle X\rangle$, vztahem
\begin{equation}
\mathbb{E}\left[X\right] = \int_{-\infty}^{\infty}xp(x) \dd x
\end{equation}
\end{definition}
\begin{definition}{(Rozptyl náhodné veličiny)}
Má-li náhodná veličina $X$ spojitou hustotu pravděpodobnosti $p(x)$, definujeme rozptyl (varianci)  $\mathbb{D}\left[X\right]$, alternativně značeno $var(X)$, vztahem
\begin{equation}
\mathbb{D}\left[X\right] = \int_{-\infty}^{\infty}x^2p(x) \dd x = \int_{-\infty}^{\infty} \left(x - \mathbb{E}\left[X\right]\right)^2 \dd x
\end{equation}
\end{definition}
\begin{definition}{(Entropie)}
Má-li náhodná veličina $X$ spojitou hustotu pravděpodobnosti $p(x)$, definujeme entropii náhodné veličiny  $\mathbb{H}\left[X\right]$ vztahem
\begin{equation}
\mathbb{H}\left[X\right] = - \int_{-\infty}^{\infty}p(x) \ln p(x) \dd x
\end{equation}
\end{definition}



V následujícím textu uvedeme jednotlivá rozdělení a pro přehlednost jejich charakteristiky, které v této práci využíváme.  

\subsubsection{Rovnoměrné rozdělení}
Začněme jedním z nejjednodušších rozdělení. Rovnoměrné rozdělení, někdy také uniformní, přiřazuje všem hodnotám stejnou pravděpodobnost.
Je definováno na intervalu $\left( a,b \right)$ a můžeme ji vyjádřit následujícím způsobem.
 \begin{equation}
    U(a,b) =
    \begin{cases}
      \frac{1}{b-a}, & \text{pro}\ x \in (a,b) \\
      0, & \text{jinak}
    \end{cases}
  \end{equation}

\begin{itemize}
\item $\mathbb{E}\left[X\right] = \frac{1}{2}\left(a+b\right)$
\item  $\mathbb{D}\left[X\right] = \frac{1}{12}\left(b-a\right)^2$
\item $\mathbb{H}\left[X\right] = \ln{\left(b-a\right)}$
\end{itemize}

\subsubsection{Normální rozdělení}
Nejdůležitější hustota pravděpodobnosti pro spojité proměnné se nazývá normální nebo také Gaussovo rozdělení. Jeho hustota je definována $\forall x \inR $ pomocí dvou parametrů $\mu \in \R$ a $\sigma^2 > 0$ jako
\begin{equation}
\mathcal{N}(\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\lbrace {-\frac{(x-\mu)^2}{2\sigma^2}}\right\rbrace 
\end{equation} 

\begin{itemize}
\item $\mathbb{E}\left[X\right] = \mu$
\item  $\mathbb{D}\left[X\right] = \sigma^2$
\item $\mathbb{H}\left[X\right] = \frac{1}{2}\ln2\pi\ee\sigma^2$
\end{itemize}
 Budeme využívat i d-rozměrnou variantu Gaussova rozdělení, které je definováno vztahem
\begin{equation}
\mathcal{N}(\mu,\Sigma) = \frac{1}{\sqrt{(2\pi)^d \lvert \Sigma \rvert}}\exp\left\lbrace{-\frac{1}{2}(\textbf{x}-\boldsymbol{\mu})^{T}\Sigma^{-1}(\textbf{x}-\boldsymbol{\mu})}\right\rbrace,
\end{equation}
kde $\Sigma$ je d$\times$d matice, kterou nazveme kovarianční a $\boldsymbol{\mu}$ je vektor středních hodnot.
\begin{itemize}
\item $\mathbb{E}\left[X\right] = \boldsymbol{\mu}$
\item  $\mathbb{D}\left[X\right] = \Sigma$
\item $\mathbb{H}\left[X\right] = \frac{1}{2}\ln \det \left(2\pi\ee\Sigma\right)$
\end{itemize}
\subsubsection{Gamma rozdělení}
Gamma rozdělení je definováno stejně jako normální rozdělení pomocí dvou parametrů $\alpha > 0$ a $\beta > 0$. Jeho hustota pravděpodobnosti má smysl pro $\forall x > 0$ a můžeme ji najít v několika možných tvarech. My uvedeme tento:
\begin{equation}
\Gamma(\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma (\alpha)}x^{\alpha -1}\exp\left\lbrace {-\beta x}\right\rbrace 
\end{equation} 
Stejně jako u Gaussova rozdělení uvedeme některé důležité charakteristiky.
\begin{itemize}
\item $\mathbb{E}\left[X\right] = \frac{\alpha}{\beta} $
\item  $\mathbb{D}\left[X\right] = \frac{\alpha}{\beta^2} $
\item $\mathbb{H}\left[X\right] = \alpha - \ln\beta + \ln \Gamma(\alpha) + (1-\alpha)\psi(\alpha)$
\end{itemize}
\subsubsection{Inverzní gamma rozdělení}
Inverzní gamma rozdělení je gamma rozdělení akorát pro převrácenou hodnotu $x$, je tedy opět popsáno dvěma parametry $\alpha > 0$ a $\beta > 0$ a definováno pro $\forall x > 0$. Jeho hustotu můžeme zapsat následovně:
\begin{equation}
i\Gamma(\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma (\alpha)}x^{-\alpha -1}\exp\left\lbrace{-\frac{\beta}{ x}}\right\rbrace 
\end{equation} 
Střední hodnota a rozptyl i$\Gamma(\alpha,\beta)$ nejsou ale definována pro $\alpha > 0$, platí:
\begin{itemize}
\item $\mathbb{E}\left[X\right] = \frac{\beta}{\alpha - 1} $, pro  $\alpha > 1$
\item  $\mathbb{D}\left[X\right] =  \frac{\beta^2}{(\alpha -1)^2(\alpha - 2)^2}$, pro $\alpha > 2$
\item $\mathbb{H}\left[X\right] =\alpha + \ln\beta + \ln \Gamma(\alpha) - (1+\alpha)\psi(\alpha)$
\end{itemize}

\clearpage

\subsection{Bayesovská metoda nejmenších čtverců}
Uvažujme standardní problém na lineární regresi \eqref{eq:regresematrix}, avšak více specifikujme šum $\varepsilon_i$, kterými je zatížen každý bod $y_i$ pro $\forall i \in  \hat{n}$ a to následovně
\begin{equation}\label{eq:bayeslinearregression}
\textbf{y} = \mathbb{X} \cdot {\theta} + \epsilon,
\end{equation}

Platí $\epsilon = (\varepsilon_1, \varepsilon_2,\dots, \varepsilon_n)$ pro jehož složky platí že jsou iid (independent and identically distributed) s rozdělením $\varepsilon_i \sim \mathcal{N}(0,1)$ a tudíž  $p(\varepsilon_i) \propto \exp\left\lbrace -\frac{1}{2}\varepsilon_i^2\right\rbrace $. 
\begin{poz}
Zanedbáváme normalizační konstantu hustot, proto využíváme znak úměrnosti $\propto$.
\end{poz}

Z rovnice \eqref{eq:bayeslinearregression} jednoduchou úpravou dostaneme
\begin{equation}
\epsilon = \textbf{y} -  \mathbb{X} \cdot {\theta}
\end{equation}
Této rovnici odpovídá následující přepis pomocí hustot, přesněji vícerozměrného Gaussova rozdělení

\begin{equation}
 p(\epsilon) \propto p(\textbf{y}|\mathbb{X},\theta) \propto \exp\left\lbrace -\frac{1}{2}(\textbf{y} - \mathbb{X}\theta)\tran(\textbf{y} - \mathbb{X}\theta)\right\rbrace 
\end{equation}
Snažíme se získat hustotu $p(\theta|\textbf{y},\mathbb{X})$, kterou získáme pomocí Bayesovy věty \eqref{Bayes}.
\begin{equation}
p(\theta|\textbf{y},\mathbb{X}) = \frac{p(\textbf{y}|\mathbb{X},\theta)p(\theta|\mathbb{X})}{p(\textbf{y}|\mathbb{X})} \propto p(\textbf{y}|\mathbb{X},\theta)p(\theta|\mathbb{X}).
\end{equation}
K tomu abychom mohli pokračovat ve výpočtu $p(\theta|\textbf{y},\mathbb{X})$, potřebujeme určit $p(\theta|\mathbb{X})$. Jelikož je $\theta$ nezávislé na $\mathbb{X}$, můžeme psát pouze $p(\theta)$.\\
Pro hustotu $p(\theta)$ předpokládáme následující vztah:
\begin{equation}
 p(\theta) = \mathcal{N}\left( 0,\alpha^{-1}\mathbb{I}\right)  \propto \exp\left\lbrace -\frac{1}{2}\theta\tran\theta\alpha\right\rbrace 
\end{equation}
Nyní můžeme pokračovat dosazením do (2.8):
\begin{equation}\label{eq:upravybayes}
\begin{split}
p(\textbf{y}|\mathbb{X},\theta)p(\Theta|\mathbb{X})  & \propto  \exp\left\lbrace -\frac{1}{2}(\textbf{y} - \mathbb{X}\theta)\tran(\textbf{y} - \mathbb{X}\theta)\right\rbrace  \exp\left\lbrace -\frac{1}{2}\theta\tran\theta\alpha\right\rbrace  \\
 & \propto
\exp\left\lbrace -\frac{1}{2}\left( \textbf{y}\tran\mathbb{Y}-\theta\tran\mathbb{X}\tran\textbf{y}-\mathbb{Y}\tran\mathbb{X}\theta+\theta\tran\mathbb{X}\tran\mathbb{X}\theta+\theta\tran\theta\alpha\right) \right\rbrace  \\
 & \propto
 \exp\left\lbrace -\frac{1}{2}\left[ \textbf{y}\tran\textbf{y}-\theta\tran\mathbb{X}\tran\textbf{y}-\textbf{y}\tran\mathbb{X}\theta + \theta\tran\left( \mathbb{X}\tran\mathbb{X} + \alpha \mathbb{I}\right) \theta\right] \right\rbrace 
\end{split}
\end{equation}
Pro dokončení je důležitý předpoklad tvaru řešení a to:
\begin{equation*}
p(\theta|\textbf{y},\mathbb{X}) \propto \exp\left\lbrace -\frac{1}{2}\left(\theta-\hat{\theta}\right)\Sigma^{-1}\left(\theta-\hat{\theta}\right) + z\right\rbrace  \propto \exp\left\lbrace -\frac{1}{2}\left(\theta-\hat{\theta}\right)\Sigma^{-1}\left(\theta-\hat{\theta}\right)\right\rbrace \exp\left\lbrace z\right\rbrace 
\end{equation*}
který dále pomocí prvního tvaru upravíme tak, abychom dokázali určit $\hat{\theta}$, $\Sigma$ a $z$. Roznásobením dostaneme
\begin{equation}\label{upravybayes2}
p(\theta|\textbf{y},\mathbb{X}) \propto \exp\left\lbrace -\frac{1}{2}\left(\theta^{T}\Sigma^{-1}\theta - \hat{\theta}^{T}\Sigma\theta -\theta^{T}\Sigma^{-1} \hat{\theta} +  \hat{\theta}^{T}\Sigma^{-1} \hat{\theta}\right) + z\right\rbrace 
\end{equation}
z čehož už při porovnání výrazu $\theta\tran\left( \mathbb{X}\tran\mathbb{X} + \alpha \mathbb{I}\right) \theta$ v konečném tvaru rovnice \eqref{eq:upravybayes} s výrazem $\theta\tran\Sigma^{-1} \theta$ v předchozí rovnici \eqref{upravybayes2}, plyne předpis pro 
\begin{equation}
\Sigma^{-1} = \mathbb{X}\tran\mathbb{X} + \alpha \mathbb{I}
\end{equation}
Tento je výsledek je pro nás velmi důležitý a budeme jej i nadále využívat. \\
 Přímo porovnávejme další dva výrazy z těchto rovnic
\begin{equation*}
-\textbf{y}\tran\mathbb{X}\theta = -\hat{\theta}\tran\Sigma^{-1}\theta
\end{equation*}
Nyní z této rovnice jednoduchou úpravou a dosazením za $\Sigma$ dostaneme další velmi důležitý předpis pro $\hat{\theta}$, a to 
\begin{equation}
\hat{\theta} = \Sigma\mathbb{X}\tran\textbf{y} = \left(\mathbb{X}\tran\mathbb{X} + \alpha \mathbb{I}\right)^{-1}\mathbb{X}\tran\textbf{y}
\end{equation}
Pro $z$ nám zbývá
\begin{equation*}
z = \textbf{y}\tran\textbf{y} - \hat{\theta}\tran\Sigma^{-1} \hat{\theta}\tran.
\end{equation*}
Používáme ale znak úměrnosti a $\exp\left\lbrace z\right\rbrace $ je pouze konstanta, můžeme ji tedy vynechat a dostaneme 
\begin{equation*}
 p(\theta|\textbf{y},\mathbb{X}) \propto \exp\left\lbrace -\frac{1}{2}\left(\theta-\hat{\theta}\right)\Sigma^{-1}\left(\theta-\hat{\theta}\right)\right\rbrace 
\end{equation*}



\subsection{Divergence}
Divergence  je funkce $D(.\Vert.)$ : $S\times S$ $\to$ $\mathcal{R}$, kde je $S$ je prostor pravděpodobnostních rozdělení a které splňuje následující dvě podmínky:
\begin{enumerate}
\item  $D(q\Vert p) \geq 0$
\item  $D(q\Vert p) = 0$ pro $p=q$
\end{enumerate}
Divergence do jisté popisuje vzdálenost nebo rozdíl mezi dvěma distribucemi. Jelikož divergence nemusí splňovat podmínku symetrie a trojúhelníkové nerovnosti, nejedná se tedy o metriku, nýbrž o semimetriku.
\subsubsection*{f-divergence}
Nejdůležitější skupinou divergencí jsou takzvané f-divergence. Jsou definovány pomocí konvexní funkce $f(x)$, kde $x>0$ a takové že $f(1) = 0$. Jsou tvaru
\begin{equation}
 D_f(q\Vert p) = \int_{supp(q)} q(x)f{\left(\frac{q(x)}{p(x)}\right)}       \dd x
\end{equation}
kde $supp(q)$ značí nosič funkce $q(x)$.
\subsubsection*{Kullback-Leiblerova divergence}
Pro nás bude užitečná tzv. Kullback-Leiblerova divergence, kde za funkci $f$ bereme přirozený logaritmus, značeno $\log$. To je rozhodně konvexní funkce pro kterou platí $\log{1} = 0$. Tvar KL-divergence je následující:
\begin{equation}\label{KLdivergence}
 D_{KL}(q\Vert p) = \int_{supp(q)} q(x)\log{\frac{q(x)}{p(x)}}       \dd x
\end{equation}
\subsection{ELBO}
Předpokládejme že máme pozorování $X$ a $Z$ jsou skryté (latentní) proměnné. Posteriorní distribuci latentní proměnné $Z$ můžeme napsat pomocí Bayesova pravidla \eqref{Bayes}, jehož jmenovatel se někdy také nazývá evidence,  takto:
\begin{equation}
p(Z\vert X) =\frac{p(X\vert Z)p(Z)}{p(X)}= \frac{p(X\vert Z)p(Z)}{\int p(X,Z) \dd Z}
\end{equation}
 Dále zadefinujeme nový objekt
\begin{equation}
\log p(X) = \log\int p(X,Z) \dd Z
\end{equation}
a abychom mohli pokračovat, využijeme pomocnou funkci $q(Z\vert \theta)$
\begin{equation}
\log p(X) = \log\int p(X,Y)\dd Z  = \log\int q\left(Z\vert \theta \right) \frac{p(X,Z)}{q\left(Z\vert\theta \right)}\dd Z = \log\mathbb{E}_q\left[\frac{p(X,Z)}{q\left(Z\vert\theta \right)}\right]
\end{equation}
Dále využijeme Jensenovu nerovnost, díky které získáme spodní hranici (lower bound), odtud Evidence Lower Bound, čili ELBO.
\begin{equation}
\log\mathbb{E}_q\left[\frac{p(X,Z)}{q\left(Z\vert\theta \right)}\right] \geq \mathbb{E}_q\left[\log\frac{p(X,Z)}{q\left(Z\vert\theta \right)}\right]
\end{equation}
ELBO můžeme rozepsat pomocí součinového pravidla \eqref{chainrule}, využít vlastností logaritmů a dle definice KL-divergence \eqref{KLdivergence}, přepsat do tvaru
\begin{align}
\mathbb{E}_q\left[\log\frac{p(X,Z)}{q\left(Z\vert\theta \right)}\right] & =  \mathbb{E}_q\left[\log\frac{p(X\vert Z)p(Z)}{q\left(Z\vert\theta \right)}\right] = \mathbb{E}_q\left[\log p(X\vert Z)\right] - \mathbb{E}_q\left[\log\frac{p(Z)}{q\left(Z\vert\theta \right)}\right] \\ & =  \mathbb{E}_q\left[\log p(X\vert Z)\right] - D_{KL}\left(q\left(Z\vert\theta \right) \Vert p(Z)\right) = \mathcal{L}\left(\theta\right)
\end{align}
Budeme-li maximalizovat ELBO přes všechny variační parametry $\theta$, získáme nejbližší možnou hodnotu k $\log p(X)$. Navíc je maximalizace ELBO ekvivalentní k minimalizaci KL-divergence mezi $q(Z\vert\theta)$ a $p(Z\vert\theta)$, jelikož platí
\begin{equation}
\begin{split}
D_{KL}\left(q\left(Z\vert\theta \right) \Vert p(Z\vert X)\right)
 & = \mathbb{E}_q\left[\log\frac{q(Z\vert\theta)}{p\left(Z\vert X \right)}\right] \\ 
& = \mathbb{E}_q\left[\log\frac{q(Z\vert\theta)p(X)}{p\left(X\vert Z\right)p(Z)}\right] \\ 
& = - \mathbb{E}_q\left[\log p(X\vert Z)\right] + \mathbb{E}_q\left[\log\frac{q(Z\vert\theta)}{p\left(Z\vert X \right)}\right] + \mathbb{E}_q\left[\log p(X)\right] \\
 & = - \mathbb{E}_q\left[\log p(X\vert Z)\right] + D_{KL}\left(q\left(Z\vert\theta \right) \Vert p(Z)\right) + \log p(X) 
\end{split}
\end{equation}
Z toho jednoduchou úpravou dostaneme konečný vztah
\begin{equation}
D_{KL}\left(q\left(Z\vert\theta \right) \Vert p(Z\vert X)\right) = - \mathcal{L}\left(\theta\right) + \log p(X)
\end{equation}



\subsubsection*{Příklad}
Předvedeme příklad, jak ELBO využít v praxi.\\
Uvažujme pouze sadu dvou pozorování $y_1$ a $y_2$ s  normálním rozdělením $\mathcal{N_\mathrm{i}}(\theta, 1)$ pro i $\in$1,2. Dále uvažujme jeden parametr $\theta$ $\sim$ $\mathcal{N}(0,\alpha)$ a nechť $\alpha$ má inverzní gamma rozdělení, tedy $\alpha$ $\sim$ $i\Gamma(0,0)$. 
Snažíme se získat sdruženou distribuci parametrů $\theta$ a $\alpha$, tedy $p(\theta, \alpha|y_1,y_2)$. Tuto distribuci můžeme přepsat pomocí definice podmíněné pravděpodobnosti \eqref{podminenapravdepodobnost} a řetězového pravidla \eqref{chainrule} jako
\begin{equation}
p(\theta, \alpha|y_1,y_2) = \frac{p(\theta, \alpha,y_1,y_2)}{p(y_1,y_2)} =  \frac{p(y_1|\theta) p(y_2|\theta)p(\theta)p(\alpha)}{p(y_1,y_2)}
\end{equation} 
Dosazením předpokladů do čitatele dostaneme:
\begin{equation}
p(y_1|\theta) p(y_2|\theta)p(\theta)p(\alpha) \propto \exp\left\lbrace -\frac{1}{2}(y_1-\theta)^2\right\rbrace \cdot  \exp\left\lbrace -\frac{1}{2}(y_2-\theta)^2\right\rbrace \cdot \frac{1}{\sqrt{\alpha}}\exp\left\lbrace -\frac{\theta^2}{2\alpha}\right\rbrace  \cdot \frac{1}{\alpha}
\end{equation}
Zdánlivě se nám může zdát určení jmenovatele jako jednoduché, protože pravdědobnost $p(y_1,y_2)$ lze získát tzv. marginalizací, nebo-li vyintegrováním přes $\theta$ a $\alpha$. 
\begin{equation}\label{prikladkl}
\begin{split}
p(y_1,y_2) & = \int p(\theta, \alpha,y_1,y_2) \dd \theta \dd\alpha\\ 
&= \int p(y_1|\theta)p(y_2|\theta)p(\theta)p(\alpha) \dd \theta \dd\alpha\\
&= \int \exp \left\lbrace -\frac{1}{2}(y_1-\theta)^2\right\rbrace \cdot  \exp\left\lbrace -\frac{1}{2}(y_2-\theta)^2\right\rbrace \cdot \frac{1}{\sqrt{\alpha}}\exp\left\lbrace -\frac{\theta^2}{2\alpha}\right\rbrace  \cdot \frac{1}{\alpha}~\dd\theta \dd\alpha
\end{split}
\end{equation}
Po bližším přezkoumání \eqref{prikladkl} zjistíme, že nelze přes $\alpha$ vyintegrovat. Proto použijeme ELBO. Dle definice KL-divergence a za předpokladu $q(\theta,\alpha) = q(\theta)q(\alpha)$ můžeme psát:
\begin{equation}
D_{KL}\left(q\left(\theta,\alpha\vert\mu,\sigma,\gamma,\delta \right) \Vert p(\theta,\alpha\vert y_1,y_2)\right)  = \int_{G}q(\alpha)q(\theta)\ln\left\lbrace \frac{q(\alpha)q(\theta)}{p(\theta,\alpha\vert y_1,y_2)}\right\rbrace  \dd\theta \dd\alpha  =\blacklozenge
\end{equation} 

kde $G = supp(q(\theta))\times supp(q(\alpha))$. Nezapomínejme, že $q(\theta)$ a $q(\alpha)$ jsou distribuce, pro které si apriori zvolíme 
 \begin{equation*}
\begin{split}
q(\theta) = \mathcal{N}(\mu , \sigma) &\\
q(\alpha) = i\Gamma(\gamma,\delta)&
\end{split}
\end{equation*}
     Dle \eqref{hustota} navíc víme, že platí $\int_{G}q(\alpha)q(\theta) \dd\theta \dd\alpha = 1$. Výraz budeme rozepisovat pomocí pravidel pro logaritmy a postupně upravovat. Výraz $p(y_1,y_2)$ v integrálu je konstanta, kterou můžeme pro jednoduchost zanedbat. Výsledek budeme na konci maximalizovat a konstanta polohu maxima nemění.

\begin{equation}
\begin{split} 
\blacklozenge & \propto \int_{G}q(\alpha)q(\theta)\ln\frac{q(\alpha)q(\theta)}{p(y_1|\theta)p(y_2|\theta)p(\theta)p(\alpha)} \dd\theta \dd\alpha \\
& = \int_{G} q(\theta)q(\alpha)\left(-\ln{p(y_1|\theta)} - \ln{p(y_2|\theta)} - \ln{p(\theta)} - \ln{p(\alpha)} + \ln{q(\theta)} + \ln{q(\alpha)}\right) \dd \alpha \dd \theta \\&
 \end{split}
\end{equation}

Poslední dva výrazy jsou tzv. entropie pro Gaussovo rozdělení, resp.  inverzní gamma rozdělení. Můžeme využít již známých výsledků:
\begin{equation*}
\begin{split}
& \int q(\theta)\ln{q(\theta)}~\dd \theta \propto - \frac{1}{2}\ln{\sigma} \\
& \int q(\alpha)\ln{q(\alpha)}~\dd \alpha = - \gamma - \ln{\delta \Gamma (\gamma)} + (1+\gamma)\psi (\gamma)  
\end{split}
\end{equation*}
Vypočítejme zbývající výrazy, kde pro jednoduchost budeme pro střední hodnoty využívat značení pomocí špičatých závorek:
\begin{equation*}
\begin{split}
 &\int_{G} q(\theta)q(\alpha)\ln{p(y_1|\theta)}~\dd \alpha~\dd \theta = \left\langle  -\frac{1}{2}(y_1 - \theta)^2\right\rangle = -\frac{1}{2}\left(y_1^2 -2y_1\mu + \mu^2 + \sigma \right) \\
 &\int_{G} q(\theta)q(\alpha)\ln{p(y_2|\theta)}~\dd \alpha~\dd \theta = \left\langle  -\frac{1}{2}(y_2 - \theta)^2\right\rangle = -\frac{1}{2}\left(y_2^2 -2y_2\mu + \mu^2 + \sigma \right) \\
 &\int_{G} q(\theta)q(\alpha)\ln{p(\theta)}~\dd \alpha~\dd \theta = \left\langle -\frac{\theta^2}{2\alpha} -\frac{1}{2}\ln{\alpha}  \right\rangle  = -\frac{1}{2}\left(\left(\mu^2 + \sigma  \right)\frac{\gamma}{\delta} + \ln{\delta}- \psi (\gamma) \right) \\
 &\int_{G} q(\theta)q(\alpha)\ln{p(\alpha)}~\dd \alpha~\dd \theta = \left\langle - \ln{\alpha}\right\rangle   = \psi (\gamma) - \ln{\delta}
\end{split}
\end{equation*}
Nyní máme všechny výrazy pro výpočet distribuce $q(\theta,\alpha)$ numericky a to pomocí maximalize KL-divergence přes parametry $\mu,\sigma,\gamma,\delta$.

\section{Teorie grafů}
Pro popis složitějších datových struktur, zejména těch stromových , můžeme využít teorie grafů. Strom je totiž speciální případ grafu. Pokusme se to řádně zadefinovat.
\begin{definition}{(Graf)}
Grafem $G$ se rozumí dvojice $(V,H)$, kde $V$ je množina vrcholů grafu $G$, $H$ je množina hran tohoto grafu a tyto množiny jsou vzájemně disjunktní.
\end{definition}
\begin{definition}{(Cesta v grafu)}
Cestou v grafu rozumíme posloupnost vrcholů a hran  $(v_0, h_1, v_1,\dots ,h_t, v_t) $, kde vrcholy $v_0,\dots,v_t$ jsou navzájem různé vrcholy grafu $G$ a pro každé $i = 1,2,\dots,t$ je $e_i = \left\lbrace v_{i-1}, v_i\right\rbrace   \in H$
\end{definition}
\begin{definition}{(Souvislost grafu)}
Řekneme že graf $G$ je souvislý, jestliže pro každé dva vrcholy $v_0$ a $v_1$ existuje v $G$ cesta z $v_0$ do $v_1$.
\end{definition}
\begin{definition}{(Cyklus v grafu)}
Cyklem v grafu $G$ rozumíme posloupnost vrcholů a hran $(v_0, h_1, v_1,\dots ,h_t, v_t~=~v_0)$, kde vrcholy $v_0,\dots,v_{t-1}$ jsou navzájem různé vrcholy grafu $G$a pro každé $i = 1,2,\dots,t$ je $e_i = \left\lbrace v_{i-1}, v_i\right\rbrace   \in~H$
\end{definition}
\begin{definition}{(Strom)}
Strom je souvislý graf neobsahující cyklus.
\end{definition}
V podstatě si to můžeme představit opravdu jako strom - má jeden kořen, v první úrovni se dělí na $K_1$ větví, každá další větev se v druhé úrovni dělí na $K_{2_i}$ a tak dále.  My se budeme v této práci zabývat pouze kořenem a první úrovní větví. 









\begin{comment}
\newpage  

Pokusme se nyní využít KL - divergenci v poněkud složitějším případě. Nalezněme koeficienty polynomu pátého stupně, který nejlépe proloží dva body.\\ Uvažujme následující pravděpodobnostní model
\begin{equation*}
 P(\textbf{y},\theta \vert X,\alpha) = P(\textbf{y}\vert \theta ,X)P(\theta \vert \alpha)P(\alpha) = \mathcal{N}(X\theta, I)\mathcal{N}(0,\alpha^{-1}I)\Gamma(0,0)
 \end{equation*}
 
Dále k hledání minima využijme KL divergenci  a aproximační distribuce
\begin{equation*}
\begin{split}
q(\theta) = \mathcal{N}(\hat{\theta} , \Sigma) &\\
q(\alpha) = \Gamma(\gamma,\delta)&
\end{split}
\end{equation*}
KL divergence je tedy tvaru
\begin{equation*}
\begin{split}
KL(q\Vert p) & =  \int q(\theta)q(\alpha)\ln{\frac{q(\theta)q(\alpha)}{P(\textbf{y}\vert \theta ,X)P(\theta \vert \alpha)P(\alpha)}}\dd \alpha \dd \theta \\ &
= \int q(\theta)q(\alpha)\left(\ln{q(\theta)} + \ln{q(\alpha)} - \ln{P(\textbf{y}\vert \theta ,X)} - \ln{P(\theta \vert \alpha)} - \ln{P(\alpha)}\right) \dd \alpha \dd \theta \\&
\end{split}
\end{equation*}
Následující členy jsou opět záporně vzaté entropie jednotlivých distribucí, víme tedy že:
\begin{equation*}
\begin{split}
& \int q(\theta)\ln{q(\theta)}~\dd \theta \propto - \frac{1}{2}\ln{|\Sigma|} \\
& \int q(\alpha)\ln{q(\alpha)}~\dd \alpha = - \gamma - \ln{\delta} + \Gamma (\gamma) + (1-\gamma)\psi (\gamma)  
\end{split}
\end{equation*}
Ostatní členy budeme nyní řešit zároveň: 

\begin{equation*}
\begin{split}
\bigstar & =  \int q(\theta)q(\alpha)\left( - \ln{P(\textbf{y}\vert \theta ,X)} - \ln{P(\theta \vert \alpha)} - \ln{P(\alpha)}\right) \dd \alpha \dd \theta \\
& = \left\langle  \frac{1}{2}\left( \left(\textbf{y} -X\theta\right)\tran \left(\textbf{y} -X\theta\right) +\alpha\theta\tran\theta - 5\ln{\alpha}\right)- \ln{\alpha}\right\rangle \\
& = \left\langle \frac{1}{2} \left( \textbf{y}\tran\textbf{y} - \theta\tran X\tran\textbf{y} -\textbf{y}\tran X \theta + \theta\tran X\tran X\theta + \alpha\theta\tran\theta - 7\ln{\alpha}     \right)     \right\rangle \\
& = \left\langle \frac{1}{2}\left( \textbf{y}\tran\textbf{y} - \theta\tran X\tran\textbf{y} -\textbf{y}\tran X \theta + \tr\left(\theta\tran X\tran X\theta + \alpha\theta\tran\theta \right) - 7\ln{\alpha}     \right) \right\rangle \\
& = \left\langle \frac{1}{2}\left( \textbf{y}\tran\textbf{y} - \theta\tran X\tran\textbf{y} -\textbf{y}\tran X \theta + \tr\left( X\tran X\theta\theta\tran + \alpha\theta\theta\tran \right) -7\ln{\alpha}     \right) \right\rangle \\
& = \left\langle \frac{1}{2}\left( \textbf{y}\tran\textbf{y} - \theta\tran X\tran\textbf{y} -\textbf{y}\tran X \theta + \tr \left( \left( X\tran X + \alpha I\right) \theta\theta\tran \right) - 7\ln{\alpha}     \right) \right\rangle 
\end{split}
\end{equation*}
Po výpočtu středních hodnot dostaneme konečný výsledek, který je tvaru:
\begin{equation*}
\bigstar = \frac{1}{2}\left( \textbf{y}\tran\textbf{y} - \hat{\theta}\tran X\tran\textbf{y} -\textbf{y}\tran X \hat{\theta} + \tr \left( \left( X\tran X + \frac{\gamma}{\delta} I\right) \left(\hat{\theta}\hat{\theta}\tran + \Sigma\right)\right)  -7\left(\psi(\gamma) - \ln{\delta} \right)\right) 
\end{equation*}
Tímto máme vypočteny všechny výrazy pro optimalizaci pomocí gradientní metody.


\end{comment}


\chapter{Generativní modely}
Ve strojovém učení se setkáváme s dvěma hlavními typy modelů a to jsou generativní modely a diskriminativní modely. Jak už napovídá název této práce, budeme se zde zabývat výhradně generativními modely.
\begin{definition}{(Generativní model)}
Mějme nějaká množinu datových záznamů $X$ a nějakou množinu  $Y$. Cílem je tuto množinu $X$ klasifikovat pomocí množiny $Y$. Generativní model je potom takový model, který se učí sdruženou distribuci $p(X,Y)$.
\end{definition}
\subsubsection*{Příklad}
Jeden ze způsobů jak odhadnout distribuci $p(Y,X)$ je využití součinového pravidla \eqref{chainrule}, pomocí kterého získáme
\begin{equation}
p(Y,X) = p(Y\vert X)p(X)
\end{equation}
Problém je tedy převeden na hledání distribucí $p(Y\vert X)$ a $p(X)$. Pro ilustraci uvažujme následující množinu datových záznamů.\\
\begin{figure}[h]
\includegraphics[width=12cm]{Images/TITLE/salam}
\centering
\end{figure}
\newpage
Určit distribuci $p(X)$ není nic těžkého, jelikož jsou tato data na ose x rozděleny rovnoměrně. To můžeme určit například z histogramu x-ových souřadnic jednotlivých bodů.
Ten vypadá následovně:\\
\begin{figure}[h]
\includegraphics[width=14cm]{Images/TITLE/p(x)}
\centering
\caption{Normalizovaný histogram x-ových souřadnic a jeho distribuce $p(X)$. }
\end{figure}
\\Histogram je znormalizován a oranžovou čarou je zde znázorněno rovnoměrné rozdělení $p(X)=U(-1,2)$.
\\Nyní přejdeme k hledání distribuce $p\left(Y\vert X \right)$. Tu můžeme určit pomocí metody nejmenších čtverců \eqref{regresethetahat}, protože víme že pro takovou distribuci platí $p(Y\vert X) = \mathcal{N}\left(X\theta, \sigma^2 I \right)$, kde $\sigma^2$ je rozptyl šumu $\varepsilon_i$.


\section{Variační autoencoder}
Cílem  je najít hustotu $p(x)$ vzorků $\left\lbrace x^{(i)} \right\rbrace^N_{i=1} $, jehož empirická hustota se dá zapsat pomocí delta funkce
\begin{equation}
p_{\mathrm{emp}}(x) = \frac{1}{N}\sum^N_{i=1}\delta (x-x^{(i)})
\end{equation}
Předpokládáme následující vztahy $x = f_{\theta}(z) + \epsilon$, $\epsilon \sim \mathcal{N}\left(0,\sigma^2I \right)$ a vzájemnou nezávislost $x^{(i)}$. Z toho můžeme určit distribuce:
 
\begin{equation}
\begin{split}
 p(x\vert z) &= \mathcal{N}\left(f_{\theta}(z),\sigma^2I \right), \\
p(z) &= \mathcal{N}\left(0,I \right),
\end{split}
 \end{equation}
 a proto má smysl využít následující formu apromaximace
\begin{equation}
p(x) = \int p(x\vert z)p(z)\dd z
\end{equation}
\subsection{Naivní přístup}
K nalezení $p(x)$ je třeba najít parametry $\theta$ transformace $f(z)$, proto zkusme využít KL-divergence a  hledat tak $\theta$ minimalizací $D_{KL}\left(p_{\mathrm{emp}}(x) \Vert p_{\theta}(x)  \right)$
\begin{equation}
\begin{split}
\hat{\theta} & = \argmin \sum_{i=1}^N \log p\left(x^{(i)} \right) \\
& =  \argmin \sum_{i=1}^N \log \int \mathcal{N}\left(f_{\theta}(z),\sigma^2I \right)\mathcal{N}\left(0,I \right)    \dd z \\
& = \argmin \sum_{i=1}^N \ \log\sum_{j=1}^N \exp \left\lbrace -\frac{1}{2\sigma^2} \left(x - f_{\theta}(z)  \right) \right\rbrace 
\end{split}
\end{equation}
\subsection{Variační Bayseova metoda}
Lepší metodou se ukazuje vzorkovat z podmíněné distribuce $q(z\vert x)$ a využít ELBO: 

\begin{equation}
\begin{split}
D_{KL}\left(q\left(z\vert x \right) \Vert p(z\vert x)\right) & = 
 \mathbb{E}_q\left[\log q(z\vert x) - \log p\left(z\vert x \right)\right] \\
 & =  \mathbb{E}_q\left[\log q(z\vert x) - \log p(x\vert z) - \log p(z) + \log p(x)   \right]
 \end{split}
\end{equation}
Tuto rovnici můžeme přepsat pomocí KL-divergence 
\begin{equation}
\log p(x) - D_{KL}\left(q\left(z\vert x \right) \Vert p(z\vert x)\right) = \mathbb{E}_q\left[\log p(x\vert z) \right] - D_{KL}\left(q\left(z\vert x \right) \Vert p(z)\right)
\end{equation}
kde pravá strana této rovnice je lower bound $\log p(x)$.
Jestliže vybereme parametrickou formu distribuce
\begin{equation}
q\left(z\vert x \right) = \mathcal{N}\left(\mu_{\theta}(x), \sigma^2_{\phi}(x) \right)
\end{equation}
můžeme parametry $\theta$ a $\phi$ minimalizovat zároveň a to následovně:





\subsection*{Příklad}

\chapter{Stromové struktury}


\chapter*{Závěr}

\pagestyle{plain}

\addcontentsline{toc}{chapter}{Záv\v{e}r}

Text závěru....
\begin{thebibliography}{1}
\bibitem{Allen-Cahn}S. Allen, J. W. Cahn: \emph{A microscopic theory
for antiphase boundary motion and its application to antiphase domain
coarsening}. Acta Metall., 27:1084-1095, 1979.

\bibitem{CINECA}G. Ballabio et al.: \emph{High Performance Systems
User Guide}. High Performance Systems Department, CINECA, Bologna,
2005. \url{www.cineca.it}

\bibitem{rumpf3}J. Becker, T. Preusser, M. Rumpf: \emph{PDE methods
in flow simulation post processing}. Computing and Visualization in
Science, 3(3):159-167, 2000.
\end{thebibliography}

\end{document}
